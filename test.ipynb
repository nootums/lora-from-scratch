{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from LoraHelpers import LoraModule, convert_model_to_lora_model, change_lora_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3.2-1b-inst\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"llama-3.2-1b-inst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8333,  3.5809,  7.0268,  ..., -1.2470, -1.2469, -1.2468],\n",
       "        [14.2424,  4.3746,  4.5395,  ..., -1.9085, -1.9087, -1.9092],\n",
       "        [ 8.1138, -0.6880,  0.9197,  ..., -0.8815, -0.8822, -0.8828],\n",
       "        [18.7669,  7.0025,  4.4708,  ..., -0.8601, -0.8603, -0.8607],\n",
       "        [ 6.7835, 11.4848,  9.7997,  ..., -1.0438, -1.0439, -1.0437],\n",
       "        [ 8.8547, 10.7931,  5.9058,  ..., -0.3585, -0.3590, -0.3580]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"hello, hello!\\n\\nI\"\n",
    "ts = torch.tensor(tokenizer([s])[\"input_ids\"])\n",
    "model.forward(ts).logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 15339, 128009, 128000, 128009], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello\"+tokenizer.eos_token, tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model_to_lora_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (k_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (v_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (o_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=8192, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (up_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=8192, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (down_proj): LoraModule(\n",
       "            (original_module): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (lora_module): Sequential(\n",
       "              (0): Linear(in_features=8192, out_features=8, bias=False)\n",
       "              (1): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lora_alpha(model, 16.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_parameters():\n",
    "    module.requires_grad = False\n",
    "    if module.ndim==1:\n",
    "        module.data = module.data.to(torch.float32)\n",
    "        continue\n",
    "    names = name.split('.')[:-1]\n",
    "    module_pointer = model\n",
    "    module_pointer_parent = None\n",
    "    for layer in names:\n",
    "        module_pointer_parent = module_pointer\n",
    "        module_pointer = getattr(module_pointer, layer)\n",
    "    if type(module_pointer)==nn.modules.linear.Linear:\n",
    "        lora_module = LoraModule(module_pointer)\n",
    "        setattr(module_pointer_parent, names[-1], lora_module)\n",
    "        \n",
    "\n",
    "    \n",
    "    # print(type(module_pointer), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.0.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.0.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.0.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.0.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.0.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.0.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.0.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.0.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.0.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.0.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.0.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.0.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.0.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.1.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.1.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.1.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.1.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.1.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.1.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.1.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.1.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.1.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.1.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.1.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.1.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.1.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.1.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.2.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.2.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.2.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.2.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.2.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.2.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.2.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.2.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.2.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.2.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.2.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.2.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.2.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.2.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.3.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.3.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.3.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.3.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.3.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.3.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.3.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.3.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.3.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.3.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.3.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.3.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.3.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.3.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.4.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.4.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.4.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.4.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.4.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.4.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.4.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.4.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.4.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.4.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.4.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.4.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.4.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.4.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.5.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.5.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.5.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.5.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.5.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.5.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.5.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.5.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.5.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.5.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.5.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.5.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.5.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.5.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.6.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.6.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.6.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.6.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.6.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.6.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.6.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.6.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.6.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.6.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.6.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.6.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.6.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.6.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.7.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.7.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.7.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.7.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.7.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.7.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.7.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.7.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.7.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.7.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.7.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.7.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.7.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.7.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.8.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.8.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.8.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.8.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.8.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.8.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.8.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.8.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.8.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.8.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.8.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.8.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.8.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.8.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.9.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.9.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.9.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.9.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.9.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.9.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.9.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.9.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.9.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.9.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.9.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.9.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.9.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.9.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.10.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.10.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.10.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.10.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.10.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.10.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.10.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.10.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.10.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.10.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.10.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.10.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.10.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.10.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.11.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.11.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.11.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.11.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.11.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.11.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.11.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.11.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.11.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.11.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.11.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.11.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.11.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.11.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.12.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.12.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.12.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.12.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.12.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.12.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.12.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.12.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.12.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.12.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.12.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.12.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.12.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.12.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.13.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.13.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.13.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.13.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.13.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.13.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.13.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.13.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.13.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.13.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.13.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.13.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.13.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.13.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.14.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.14.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.14.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.14.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.14.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.14.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.14.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.14.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.14.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.14.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.14.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.14.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.14.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.14.mlp.down_proj.lora_module.1.weight\n",
      "model.layers.15.self_attn.q_proj.lora_module.0.weight\n",
      "model.layers.15.self_attn.q_proj.lora_module.1.weight\n",
      "model.layers.15.self_attn.k_proj.lora_module.0.weight\n",
      "model.layers.15.self_attn.k_proj.lora_module.1.weight\n",
      "model.layers.15.self_attn.v_proj.lora_module.0.weight\n",
      "model.layers.15.self_attn.v_proj.lora_module.1.weight\n",
      "model.layers.15.self_attn.o_proj.lora_module.0.weight\n",
      "model.layers.15.self_attn.o_proj.lora_module.1.weight\n",
      "model.layers.15.mlp.gate_proj.lora_module.0.weight\n",
      "model.layers.15.mlp.gate_proj.lora_module.1.weight\n",
      "model.layers.15.mlp.up_proj.lora_module.0.weight\n",
      "model.layers.15.mlp.up_proj.lora_module.1.weight\n",
      "model.layers.15.mlp.down_proj.lora_module.0.weight\n",
      "model.layers.15.mlp.down_proj.lora_module.1.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 9906], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/adi/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Helloendetuptordoustopl RTS+=\"usto sync FocusSuffixaby evacuate afterwards afterward surely Caryucks'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.generate(torch.tensor(tokenizer(\"Hello\")[\"input_ids\"]).unsqueeze(0))[0]\n",
    "\"\".join([tokenizer.decode(token) for token in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lora_alpha(model, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/adi/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Hello, I'm excited to be a part of this community. I'm looking for a new\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.generate(torch.tensor(tokenizer(\"Hello\")[\"input_ids\"]).unsqueeze(0))[0]\n",
    "\"\".join([tokenizer.decode(token) for token in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
